\documentclass[11pt, letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{amsmath, amssymb}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{hyperref}

% Make parindent 0
\setlength{\parindent}{0pt}
\setlist{itemsep=0.5em}
\setlength{\parskip}{0.5em}

\geometry{margin=1in}
\pagestyle{fancy}
\fancyhf{}
\lhead{\textbf{Student Name:} \textcolor{red}{YOUR NAME}} % CHANGE NAME HERE
\rhead{\textbf{Data Analysis and Regression, Homework 1}}
\cfoot{\thepage}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

\newcommand{\problem}[1]{\section*{Problem #1}}
\newcommand{\subproblem}[1]{\subsection*{Problem #1}}
\newcommand{\answer}{\textbf{\textit{\textcolor{red}{Answer:}}}\\}

\begin{document}

\section*{Instructions}
\begin{itemize}
    \item Homework 2 is due December 7th at 16:00 Chicago Time.
    \begin{itemize}
        \item We will not accept any submissions past 16:00:00, even if they are only one second late.
    \end{itemize}
    \item You \textbf{must} upload the following files to the class Canvas:
    \begin{itemize}
        \item \texttt{LASTNAME\_FIRSTNAME.pdf}
        \item \texttt{LASTNAME\_FIRSTNAME.ipynb}
    \end{itemize}
    \item Your code notebook \textbf{must} be runnable using my environment outlines in class 1 (Python 3.14, and the \texttt{requirements.txt}).
    \item You \textbf{must} use this template file and fill out your solutions for the written portion.
    \item Please note that your last name and first name should match what you appear on Canvas as.
    \item Include code snippets where required, as well as math and equations.
    \item Be \textit{concise} where possible, all of the homework probelms can be answered in a few lines of math, code, and words.
\end{itemize}
\hrule
\vspace{0.5cm}

\pagebreak

\problem{1: Hands-On OLS}

\subproblem{1.1: Setup}

Set your random seet using \texttt{np.random.seed(1)}. Generate $n=30$ observations where:
\begin{itemize}
    \item The predictor $X$ is drawn from a standard normal distribution, $X \sim N(0, 1)$.
    \item The error term $\epsilon$ is drawn from a normal distribution with mean 0 and standard deviation 1, $\epsilon \sim N(0, 1)$.
    \item The response variable is generated by the true relationship: $Y = 5 + 2X + \epsilon$.
\end{itemize}

Display a scatter plot of the generated data points $(X_i, Y_i)$.

\answer

\subproblem{1.2: A First Fit}

Using the data generated above, fit an OLS model. You should report:
\begin{enumerate}
    \item $\hat{\beta}_0$ and $\hat{\beta}_1$ estimates.
    \item Your confidence intervals and p-values for both coefficients.
    \item The $R^2$ value of your fit.
\end{enumerate}

\answer

\subproblem{1.3: Interpretation}

How do your estimated coefficients and confidence intervals compare to the true parameters?

\answer

\subproblem{1.4: An Influential Point}

Now, modify your dataset by overriding the last obvervation to be the point $(X_{30}, Y_{30}) = (4, -5)$.
Refit your OLS model to this modified dataset, and report:
\begin{enumerate}
    \item $\hat{\beta}_0$ and $\hat{\beta}_1$ estimates.
    \item Your confidence intervals and p-values for both coefficients.
    \item The $R^2$ value of your fit.
\end{enumerate}

\subproblem{1.5: Interpretation}

How do your estimated coefficients and confidence intervals compare to the true parameters?

\answer

\subproblem{1.6: Cook's Distance}

Calculate Cook's distance for all observations in the modified dataset from part (d).
Plot the Cook's distance values, and highlight the 31st observation. 

\answer

\subproblem{1.7: What if?}

Suppose instead that the influential point you just added was $(X_{30}, Y_{30}) = (0, -5)$.

What would you expect the \textit{leverage} of this point to be relative to the $(4, -5)$ point you added before?

\answer


\problem{2: Central Limit Theorem}

We found in Class 2, that if $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$,
then as $n \to \infty$, then $\beta_{\text{OLS}} \to \mathcal{N}(\beta_{\text{OLS}}, \sigma_{\text{OLS}}^2)$, where $\sigma_{\text{OLS}}^2 = \sigma^2 / \sum_{i=1}^n (x_i - \bar{x})^2$.

The goal of this problem is for you to empirically verify that this 
results holds \textit{even if} $\epsilon_i$ are not normally distributed.

\subproblem{2.1: Setup}

First, define the true model as:

$$
y = 2x
$$

Where $x$ is sampled from the uniform distribution $x \sim \text{Uniform}(0, 1)$.
Note that we are not including an intercept, nor any noise.

Second, define the noise $\epsilon_i$ to be sampled from 2 different distributions:
\begin{itemize}
    \item A Bernoulli distribution with $p=0.5$, and values $\{-1, 1\}$ (the Radamacher distribution).
    \item A uniform distribution with $\epsilon_i \sim \text{Uniform}(-1, 1)$.
\end{itemize}

For $n=500$, please display histograms of the 2 noise distributions you defined above.

\answer

\subproblem{2.2: Empirical Verification}

\textbf{Note:} You should not re-sample your $x$ values, only the $\epsilon_i$ values.
That is, you should have a fixed set of $x$ values for this entire problem.

For $n \in {10, 100, 1000}$, and for each noise distribution defined above, do the following:
\begin{enumerate}
    \item Sample $n$ values of $\epsilon_i$ from the noise distribution.
    \item Generate the target values as $y_i = 2x_i + \epsilon_i$.
    \item Fit an OLS regression to the data $(x_i, y_i)$, and obtain the estimate $\hat{\beta}_{\text{OLS}}$.
    \item Repeat (1)-(3) 1,000 times, and save all of the $\hat{\beta}_{\text{OLS}}$ estimates.
\end{enumerate}
You should now have 6 sets of 1,000 $\hat{\beta}_{\text{OLS}}$ estimates (2 noise distributions $\times$ 3 values of $n$).

Include your simulation code below:

\answer

\subproblem{2.3: Visualization}

For each of the 6 sets of $\hat{\beta}_{\text{OLS}}$ estimates obtained above, plot a histogram of the estimates.
Additionally, conduct a Shapiro-Wilk test for normality on each set of estimates,
report your p-values, and briefly discuss your results.

\answer

\problem{3: Weighted Least Squares}

This problem focuses on verifying the findings of Class 2 regarding
Weighted Least Squares (WLS).

\subproblem{3.1: Setup}

Similar to before, define the true model as:
$$
y = 2x
$$
Where $x$ is sampled from the uniform distribution $x \sim \text{Uniform}(1, 2)$.

Second, define the noise $\epsilon_i$ to be sampled from a normal distribution with mean 0,
and variance $\sigma_i^2$ that depends on $x_i$ as follows:
$$
\sigma_i^2 = \frac{1}{x_i^2}
$$

For $n=500$, please plot the generated data points $(x_i + \epsilon_i, y_i)$.

\answer

\subproblem{3.2: Naive OLS}

Fit a standard OLS regression to the data generated above.

Report your estimate $\hat{\beta}_{\text{OLS}}$, and the confidence interval for the estimate.

\answer

\subproblem{3.3: Interpretation}

Briefly discuss whether naive OLS will be under or over confident in its estimate of $\hat{\beta}_{\text{OLS}}$,
and why.

\answer

\subproblem{3.4: Visualization}

Plot a Q-Q plot of the residuals from the naive OLS regression. What do you observe?

\answer

\subproblem{3.5: Weighted Least Squares}

Fit a Weighted Least Squares regression to the data generated above, using weights $w_i = x_i^2$.
Report your estimate $\hat{\beta}_{\text{WLS}}$, and the confidence interval for the estimate.

\answer

\subproblem{3.6: Visualization}

Plot a Q-Q plot of the weighted residuals from the WLS regression. What do you observe compared
to the Q-Q plot from the naive OLS regression?

\answer

\end{document}